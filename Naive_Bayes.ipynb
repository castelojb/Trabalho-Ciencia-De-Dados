{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Trabalho\n",
    "\n",
    "## Questão 1\n",
    "\n",
    "Implemente um classifacor Naive Bayes para o problema de predizer a qualidade de um carro. Para este fim, utilizaremos um conjunto de dados referente a qualidade de carros, disponível no [UCI](https://archive.ics.uci.edu/ml/datasets/car+evaluation). Este dataset de carros possui as seguintes features e classe:\n",
    "\n",
    "** Attributos **\n",
    "1. buying: vhigh, high, med, low\n",
    "2. maint: vhigh, high, med, low\n",
    "3. doors: 2, 3, 4, 5, more\n",
    "4. persons: 2, 4, more\n",
    "5. lug_boot: small, med, big\n",
    "6. safety: low, med, high\n",
    "\n",
    "** Classes **\n",
    "1. unacc, acc, good, vgood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_dataset(dataset, splitRatio):\n",
    "    msk = np.random.rand(len(df)) < splitRatio\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    return train, test\n",
    "\n",
    "def _mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    " \n",
    "def _stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "class NavieBayes:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        unique_data = [df[label].unique() for label in labels]\n",
    "        legend = []\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            _dict = {}\n",
    "            for j, cat in enumerate(unique_data[i]):\n",
    "                _dict[cat] = j\n",
    "            df[label] = df[label].map(_dict)\n",
    "            legend.append(_dict)\n",
    "\n",
    "    def _calculate_probability(self, x, mean, stdev):\n",
    "        if stdev == 0:\n",
    "            stdev = 10000\n",
    "        exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2))))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * math.pow(stdev, 2))) * exponent\n",
    "\n",
    "    def _summarize(self, dataset):\n",
    "        summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "        del summaries[-1]\n",
    "        return summaries\n",
    "    \n",
    "    def _predict_for_row(self, row_input):\n",
    "        self._calculate_class_probabilities(row_input)\n",
    "        chosen_label, chosen_prob = None, -1\n",
    "        for class_code, probability in self.probabilities.items():\n",
    "            if chosen_label is None or probability > chosen_prob:\n",
    "                chosen_prob = probability\n",
    "                chosen_label = class_code\n",
    "        return chosen_label\n",
    "     \n",
    "    def _separate_by_class(self):\n",
    "        separated = {}\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            if row[-1] not in separated:\n",
    "                separated[row[-1]] = []\n",
    "            separated[row[-1]].append(row)\n",
    "        \n",
    "        self.separated = separated\n",
    "\n",
    "    def _calculate_class_probabilities(self, row_input):\n",
    "        probabilities = {}\n",
    "        for class_value, class_summaries in self.summaries.items():\n",
    "            probabilities[class_value] = 1\n",
    "            for i, class_summary in enumerate(class_summaries):\n",
    "                mean, stdev = class_summary\n",
    "                x = row_input[i]\n",
    "                probabilities[class_value] *= self._calculate_probability(x, mean, stdev)\n",
    "        self.probabilities = probabilities\n",
    "\n",
    "    def _summarize_by_class(self):\n",
    "        self._separate_by_class()\n",
    "        summaries = {}\n",
    "        for key, data in self.separated.items():\n",
    "            summaries[key] = self._summarize(data)\n",
    "        self.summaries = summaries\n",
    "    \n",
    "    def predict(self):\n",
    "        predictions = []\n",
    "        for _, test_row in self.test_set.iterrows():\n",
    "            result = self._predict_for_row(test_row)\n",
    "            predictions.append(result)\n",
    "        self.predictions = predictions\n",
    "        return self.predictions\n",
    "    \n",
    "    def split(self, splitRatio):\n",
    "        train, test = _split_dataset(self.dataset, splitRatio)\n",
    "        self.test_set = test\n",
    "        self.train_set = train\n",
    "    \n",
    "    def fit(self):\n",
    "        self._summarize_by_class()\n",
    "        \n",
    "    def get_accuracy(self):\n",
    "        correct = 0\n",
    "        for i in range(len(self.test_set)):\n",
    "            if self.test_set.iloc[i][-1] == self.predictions[i]:\n",
    "                correct += 1\n",
    "        return (correct/float(len(self.test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"label\"]\n",
    "df = pd.read_csv('carData.csv', sep=',', names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NavieBayes(df)\n",
    "nb.split(0.7)\n",
    "nb.fit()\n",
    "\n",
    "my_predictions = nb.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.350413223140496, 1.1180986436047557),\n",
       "  (1.3669421487603306, 1.127547271846503),\n",
       "  (1.4545454545454546, 1.1271519258821532),\n",
       "  (0.7900826446280992, 0.8358593601329547),\n",
       "  (0.9322314049586777, 0.8197535262768612),\n",
       "  (0.7528925619834711, 0.8027634632071083)],\n",
       " 1: [(1.5755208333333333, 1.0419527457466646),\n",
       "  (1.5911458333333333, 1.0481986236664393),\n",
       "  (1.5859375, 1.0949254686892318),\n",
       "  (1.484375, 0.5004077971479614),\n",
       "  (1.1015625, 0.799867910929502),\n",
       "  (1.53125, 0.4996735226553633)],\n",
       " 2: [(2.6, 0.49371044145328763),\n",
       "  (2.2, 0.7541551564499178),\n",
       "  (1.7692307692307692, 1.0572551544156505),\n",
       "  (1.5384615384615385, 0.5023980952928128),\n",
       "  (1.6153846153846154, 0.4902903378454599),\n",
       "  (2.0, 0.0)],\n",
       " 3: [(2.6666666666666665, 0.4748580799338168),\n",
       "  (2.6666666666666665, 0.47485807993381696),\n",
       "  (1.565217391304348, 1.104512946553756),\n",
       "  (1.4782608695652173, 0.5031867754087856),\n",
       "  (1.0434782608695652, 0.8123093913741108),\n",
       "  (1.434782608695652, 0.4993602044724244)]}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.8030303030303"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_accuracy = nb.get_accuracy() * 100\n",
    "my_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2\n",
    "Crie uma versão de sua implementação usando as funções disponíveis na biblioteca SciKitLearn para o Naive Bayes ([veja aqui](http://scikit-learn.org/stable/modules/naive_bayes.html)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nb.train_set, nb.test_set\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "train_values, train_labels = [None] * len(train), [None] * len(train)\n",
    "test_values, test_labels = [None] * len(test), [None] * len(test)\n",
    "\n",
    "for i in range(len(train)):\n",
    "    row = train.iloc[i]\n",
    "    train_values[i] = row[:-1]\n",
    "    train_labels[i] = row[-1]\n",
    "\n",
    "nb_model.fit(train_values, train_labels)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    row = test.iloc[i]\n",
    "    test_values[i] = row[:-1]\n",
    "    test_labels[i] = row[-1]\n",
    "\n",
    "nb_model.fit(train_values, train_labels)\n",
    "\n",
    "their_predictions = nb_model.predict(test_values)\n",
    "\n",
    "their_accuracy = accuracy_score(test_labels, their_predictions) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.56060606060606"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3\n",
    "\n",
    "Analise a acurácia dos dois algoritmos e discuta a sua solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferência de acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualemente a diferença da acurácia é de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7575757575757649%'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{}%'.format(abs(their_accuracy - my_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores se manteram dentro do intervalo de 0.5% a 7%. Sendo que ambas as acurácias se manteram no intervalo de 68% a 72%, aproximadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a função `classification_report` da biblioteca _sklearn_ para calcular as métricas relacionadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementação própia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78       360\n",
      "          1       0.56      0.81      0.66       125\n",
      "          2       0.00      0.00      0.00        18\n",
      "          3       0.21      1.00      0.35        25\n",
      "\n",
      "avg / total       0.83      0.68      0.71       528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/miniconda3/envs/jupyter/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, my_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação do SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.88      0.86       360\n",
      "          1       0.67      0.16      0.26       125\n",
      "          2       0.17      1.00      0.28        18\n",
      "          3       0.60      0.24      0.34        25\n",
      "\n",
      "avg / total       0.76      0.69      0.67       528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, their_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
